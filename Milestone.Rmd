---
title: "Milestone Report"
author: "Elena Civati"
date: "5/28/2021"
output:
  html_document:
    toc: true
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message=F)
```

## Overview
The goal of this project is to build an NLP algorithm to determine which English word a user is most likely to type after digitation of "some" (a number to determine) previous words.  
The starting point is a Corpora dataset consisting of strings from blogs, online newspapers and tweets. Those strings are in English, but data in other languages are also provided and will maybe be used in further steps.  
The algorithm is initially built and tested locally, but is expected to work in a Shiny app running on the Shiny server.  
For my analysis, I used R.4.0.2 on a Windows X64 machine with 8Gb of RAM and the following libraries:
```{r echo=T}
library(ggplot2)
library(plotly)
library(dplyr)
library(xtable)
```
**NOTE**: This report is intended to be concise and understandable by a non-data scientist reader, so the vast majority of code will not be shown. If you're interested in it, you can find it on [Github](https://github.com/Elenena/NLP_Capstone) (Milestone.Rmd file). Knitting that file in RStudio (after removal of every "eval=F" option present in the file) would reproduce the entire workflow, but it could take a very long time and require you to manually clear the workspace from time to time (that's why some files are saved on hardware and then loaded again when needed). This is partly because of the nature of the analysis itself and the size of data, and partly because I discovered some tricks to improve efficiency along the way.

## Data processing and exploration
Data were downloaded from https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip
```{r eval=F}
download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "training.zip")
unzip("training.zip")

con<-file("final/en_US/en_US.blogs.txt", encoding = "UTF-8")
blogs<-readLines(con)
close(con)

con<-file("final/en_US/en_US.twitter.txt", encoding = "UTF-8")
twitter<-readLines(con)
close(con)

con<-file("final/en_US/en_US.news.txt", "rb", encoding = "UTF-8")
news<-readLines(con)
close(con)
```
I used bash command "wc -cmlwL *.txt" to find basic informations for each of the 3 en_US files in my working directory and I summarized them in a table:
<div align="center">
```{r results="asis"}
t<-read.table("datatable.txt")
print(xtable(t), type="html")
```
</div>

<br/>
Then, I merged the 3 files and, to save allocated RAM, I divided the resulting object in 50 files that were subsequently loaded one at a time for profanity filtering and tokenisation. Before splitting, strings were mixed in random order, to insure that each chunk contains strings from blogs, news and tweets in approximately the same proportion. 
I choose to convert all letters to uppercase to reduce the number of different words.  
Profanity filter was based on a bad words list found at https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/en. Profane words were replaced by the tag \<BADWORD\>.  
I also seeked for web links, email adresses, numbers, prices and dates and replaced them with tags, still in order to reduce the number of different tokens in the dataset. Those filters are probably not perfect, but they appear to find the majority of the desired items.  
Finally, lines were splitted based on whitespaces and punctuation. Again, a small percentage of words didn't split correctly, due to some strange patterns of punctuation or to typos, but this doesn't affect the quality of processed data.
```{r eval=F}
profane<-readLines("en")
corpora<-toupper(c(blogs, news, twitter))
corpora<-sample(corpora)
bounds<-round(seq(0,length(corpora), len=51))
dir.create("split")
dir.create("Tokenized")
dir.create("WordList")
for (i in 1:50) {
    saveRDS(corpora[bounds[i]+1:bounds[i+1]], paste0("split/",i,".RDS"))
}

for (i in 1:50) {
    s<-readRDS(paste0("split/",i,".RDS"))
    #Profanity filtering
    for(x in profane) {
        s<-gsub(paste0("([^A-Z]|^)",x,"([^A-Z]|$)"), "\\1 <BADWORD> \\2", s, ignore.case = T)
    }
    # Tokenization
    s<-gsub("HTTP://.+( |$)|WWW..+( |$)"," <WEBLINK> \\1",s) #web link
    s<-gsub("[A-Z0-9]{3,}@[A-Z0-9]{3,}\\.[A-Z0-9]{2,3}", " <MAIL_ADRESS> ", s) #emails
    s<-gsub("[0-9]{2}.[0-9]{2}.[0-9]{4}|[0-9]{4}.[0-9]{2}.[0-9]{2}","<DATE>",s) #date tag
    s<-gsub("([€£\\$] *)[0-9]+[//.|,]*[0-9]*", "\\1 <PRICE> ", s) #price tag
    s<-gsub("[0-9]+[//.|,]*[0-9]*( *[€£\\$])", " <PRICE> \\1", s)
    s<-gsub("( +|^|-*|\\+*)[0-9]+[//.|,]*[0-9]*( +|$)", "\\1 <NUMBER> \\2", s) #number tag
    s<-gsub("[^A-Z0-9<>]+$","",s)
    s<-gsub("^[^A-Z0-9<>]+","",s)
    
    s<-strsplit(s, "( +[^A-Z0-9<>]*)|([^A-Z0-9<>]* +)|[\\.]{2,}|[-]{2,}|\\\"|[^A-Z0-9]'")
    s<-sapply(s, function(x) x[x !=""])
    saveRDS(s, paste0("Tokenized/chunk",i,".RDS"))
    
    s<-unlist(s)
    saveRDS(s, paste0("WordList/chunk",i,".RDS"))
    
}


```
Then, I used the first 35 chunks (70% of the total, as they all have similar sizes) to be part of the training dataset, and left the other 15 for testing purpose. Chunks from 36 to 45 will be the test dataset for the final model. Chunck from 46 to 50 will be the validation dataset that will be used to test the accuracy of each of my future models. 
```{r eval=F}
temp<-list.files("WordList", full.names = T)
words<-lapply(temp[1:35], readRDS)
```


### Unigrams
Then, the 35 word lists were merged together obtaining a vector with 70,842,273 elements and tabled to determine the total number of different words (or tokens) and each word's frequency. The table was then sorted by frequency.
As shown, the total number of distinct words and tokens in the training dataset (the so-called "unigrams") is 806,759.  
As a comparison (code not provided), when I examined with the same procedure a sample containing only the 10% of the blogs file, I obtained approx 137000 words, while the total number of different tokens in the entire dataset (training + validation + test) is 1,014,175. So, as expected, increasing the number of strings examined is useful, but as the size of the dataset increases the advantage becomes smaller.  We should also consider that a great number of "new" words consists in typos.
```{r eval=F}
words<-unlist(words)
one<-as.data.frame(table(words))
one<-one[order(one$Freq, decreasing=T),]
rownames(one)<-1:nrow(one)
write.csv(one, "unigrams.csv")
```

```{r echo=T}
unigrams<-read.csv("unigrams.csv")
nrow(unigrams)
```

Now, I calculated the cumulative percent frequency and I found out that only 140 words/tokens account for the fifty percent of the entire training dataset. As shown in the following table, 3 of those are not actual words but tokens summarizing a variety of possible strings (\<NUMBER\>, \<BADWORD\>, \<PRICE\>).
```{r echo=3:4, cache=TRUE}
colnames(unigrams)<-c("ranking", "word", "frequency")
unigrams$Cumulative_frequency_Percent<-round(cumsum(unigrams$frequency/sum(unigrams$frequency)*100),2)
top50<-unigrams[unigrams$Cumulative_frequency_Percent<=50,]
nrow(top50)
```
<br/>

<div align="center">
``` {r cache=T, results="asis"}
print(xtable(top50), type="html", include.rownames=F)
```
</div>
<br/>
Now, let's see how many words are needed to cover larger fractions of the dataset:
```{r cache=T, echo=T}
top90<-unigrams[unigrams$Cumulative_frequency_Percent<=90,]
nrow(top90)
top95<-unigrams[unigrams$Cumulative_frequency_Percent<=95,]
nrow(top95)
top99<-unigrams[unigrams$Cumulative_frequency_Percent<=99,]
nrow(top99)
```
Let's look to the last tokens in the top 95% list:
```{r cache=T, echo=T}
tail(top95)

```
They seems to be not very common words, but not even so weird.

Here is a plot of the 95% coverage by the number of words used. You can zoom and hover on it to see each word with its absolute frequency in the dataset.
```{r cache=T}
g<-ggplot(top95, aes(x=ranking, y=Cumulative_frequency_Percent, label=word, label2=frequency)) + geom_point()
ggplotly(g)

```
### Digrams
Now, I created a dataframe with all the digrams (combinations of 2 words/tokens) that can be found in the training dataset and their frequencies. 
Once again, I worked with chunks to avoid exceeding memory limits of my laptop.
```{r eval=F}
digrams_gen<-function(v) {
    if (length(v)>1) {
        d<-list()
        for (i in 1:(length(v)-1)) {
            d[[i]]<-v[i:(i+1)]
        }
        d
    }
}
dir.create("digrams")
dir.create("fives")
for(i in 1:35) {
    tokenized<-readRDS(paste0("Tokenized/chunk",i,".RDS"))
    digrams<-unlist(sapply(tokenized, digrams_gen), recursive = F)
    digrams<-as.data.frame(do.call(rbind, digrams))
    digrams<- digrams %>% group_by_all() %>% summarize(Frequency=n())
    saveRDS(digrams, paste0("digrams/chunk",i,".RDS"))
}

temp<-list.files("digrams", full.names = T)
for(i in c(1,6,11,16,21,26,31)) {
    five<-lapply(temp[i:(i+4)], readRDS)
    five<-do.call(rbind,five)
    merged<-five %>% group_by(V1, V2) %>% summarize(frequency=sum(Frequency))
    saveRDS(merged, paste0("fives/chunk",i,".RDS"))
}

temp<-list.files("fives", full.names = T)
seven<-lapply(temp, readRDS)
seven<-do.call(rbind,seven)
merged<-seven %>% group_by(V1, V2) %>% summarize(Frequency=sum(frequency))
saveRDS(merged, "digrams.RDS")
```
Let's explore digrams distribution like we did with unigrams:
```{r cache=T, echo=c(1,2,5,6)}
digrams <- readRDS("digrams.RDS")
nrow(digrams)
digrams<-digrams[order(digrams$Frequency, decreasing=T),]
digrams$Cumulative_frequency_Percent<-round(cumsum(digrams$Frequency/sum(digrams$Frequency)*100),2)
top50<-digrams[digrams$Cumulative_frequency_Percent<=50,]
nrow(top50)
```

We can see that there are more than 11 millions unique digrams, and 41000 of them account for the 50% of the total. Here are the top 100 2-grams:  

<br/>  
<div align="center">
```{r results="asis", cache=T}
print(xtable(top50[1:100,]), type="html")
```
</div>
<br/>
And here is a plot with the top 1000 and cumulative frequency on the y axis:
```{r cache=T}
top50$ranking<-1:nrow(top50)
g<-ggplot(top50[1:1000,], aes(x=ranking, y=Cumulative_frequency_Percent, label=paste(V1,V2), label2=Frequency)) + geom_point()
ggplotly(g)
```


### Trigrams
The number of 3-grams is so big that my laptop couldn't manage retrieving and counting the frequency of all of them in one file. Also, as shown in the [Possible Models] section, prediction is much faster when data are split in one dataframe per letter.  
So, after an initial processing in chunks, I divided each chunk in 27 dataframes, one for each initial letter of the first word in the 3-gram, plus one for trigrams beginning with "\<", that denotes my custom tags (while doing so, I discarded a small percentage of weird tokens beginning with other characters). As those data will be the starting point for the creation of the model, I sorted them by decreasing frequency, to improve efficiency in querying the final dataframes.
```{r eval=F}
trigrams_gen<-function(v) {
if (length(v)>2) {
d<-list()
for (i in 1:(length(v)-2)) {
d[[i]]<-v[i:(i+2)]
}
d
}
}
dir.create("trigrams")
dir.create("five_tri")
dir.create("models")
for(i in 1:35) {
tokenized<-readRDS(paste0("Tokenized/chunk",i,".RDS"))
trigrams<-unlist(sapply(tokenized, trigrams_gen), recursive = F)
trigrams<-as.data.frame(do.call(rbind, trigrams))
trigrams<- trigrams %>% group_by_all() %>% summarize(Frequency=n())
saveRDS(trigrams, paste0("trigrams/chunk",i,".RDS"))
}

temp<-list.files("trigrams", full.names = T)
for(i in c(1,6,11,16,21,26,31)) {
    five<-lapply(temp[i:(i+4)], readRDS)
    five<-do.call(rbind,five)
    merged<-five %>% group_by(V1, V2, V3) %>% summarize(frequency=sum(Frequency))
    saveRDS(merged, paste0("five_tri/chunk",i,".RDS"))
}

temp<-list.files("five_tri", full.names = T)
for(j in 1:7) {
    a<-readRDS(temp[j])
    i<-grep("^[A-Z<]", a$V1)
    a<-a[i,]
    init<-strsplit(a$V1,"")
    init<-unlist(sapply(init, function(x) x[1]))
    a$factor<-as.factor(init)
    l<-split(a, a$factor)
    l<-lapply(l, function(x) x[,1:4])
    saveRDS(l[[1]], paste0("models/tags",j,".RDS"))
    for(i in 2:27) {
        saveRDS(l[[i]], paste0("models/", LETTERS[i-1], j,".RDS"))
    }
}
dir.create("models/merged")
for(i in c("tags",LETTERS)) {
    temp<-list.files("models", pattern=paste0("^",i), full.names=T)
    seven<-lapply(temp, readRDS)
    seven<-do.call(rbind,seven)
    merged<-seven %>% group_by(V1, V2, V3) %>% summarize(Frequency=sum(frequency))
    merged<-merged[order(merged$Frequency, decreasing = T),]
    saveRDS(merged, paste0("models/merged/",i,".RDS"))
    rm(seven, merged)
    gc()
}
```
Let's look at this huge collection of 3-grams:
```{r cache=T, echo=T}
temp<-list.files("models/merged",full.names=T)
trigrams<-lapply(temp, readRDS)
sum(sapply(trigrams, nrow))
```
There are nearly 34 millions different combinations of 3 words in the training dataset. Let's plot the most frequent 1000 against their frequency and table the first 100 of them.
<div align="center">
```{r cache=T, results="asis"}
trigrams<-readRDS("alldifftrigrams.RDS")
trigrams<-trigrams[1:1000,]
trigrams$ranking<-1:1000
g<-ggplot(trigrams, aes(x=ranking, y=Frequency, label=paste(V1,V2,V3), label2=Frequency)) + geom_col()
ggplotly(g)

print(xtable(trigrams[1:100,]), type="html", include.rownames=F)
```
</div>
<br/>

## Possible Models
For the modeling task, I didn't use, for the moment, any special NLP package. Basically, I'm testing my idea that a good approach would be to associate in one ore more dataframes all the observed sequences of words (of a given length) with the most frequent next word. This or those dataframes will then be filtered for the input words and the output will be the content of the last column.  
Here are some lines of code that roughly estimate the speed and memory requirements of various approaches. Note that when I wrote this code I hadn't still finished the dataframes creation, so I just used some available dataframes with the same number of columns and (roughly) the same number of rows expected in the real dataframes. That's why the prediction outputs are meaningless.

### Estimate of using ONE word as predictor

```{r cache=T, echo=T}
unigrams<-read.csv("unigrams.csv")
unigrams<-select(unigrams, words, Freq)
a<-Sys.time()
output<-filter(unigrams, words==toupper("friends")) %>% select(Freq)
b<-Sys.time()
b-a
print(object.size(unigrams), units="Mb")
```

### Estimate of using TWO words as predictors and a single dataframe

``` {r cache=T, echo=T} 
digrams <- readRDS("digrams.RDS")
a<-Sys.time()
output<-as.character(filter(digrams, V1==toupper("best") & V2==toupper("friends")))
b<-Sys.time()
b-a
print(object.size(digrams), units="Mb") 
```


### Estimate of using TWO words as predictors with multiple dataframes

``` {r cache=T, echo=T}
a<-readRDS("digrams.RDS")
i<-grep("^[A-Z]", a$V1)
a<-a[i,]
init<-strsplit(a$V1,"")
init<-unlist(sapply(init, function(x) x[1]))
a$factor<-as.factor(init)
l<-split(a, a$factor)
l<-lapply(l, function(x) x[,1:3])

a<-Sys.time()
v1=toupper("best")
v2=toupper("friends")
output<-as.character(filter(l[[substr(v1,1,1)]], V1==v1 & V2==v2))
b<-Sys.time()
b-a
print(object.size(l), units="Mb")
```

The last solution seems to be the best, assuming that prediction with 2 words is more accurate than prediction with 1 word.  
For the OOV (out-of-vocabulary) pairs of words, my algorithm will use prediction with 1 word. If not even the previous word alone is in the dictionary, the algorithm will predict the most repeated word in the dataset ("THE").  

## First prediction model
For the dataframes to be used by the prediction function, I selected for each pair of words the most frequent trigram beginning with that pair of words.
When 2 trigrams occurred the same number of times, I choose the one where the third word is more frequent in the training dataset, with the help of my list of unigrams previously created.
Same procedure for the digrams dataframe. 

### Dictionary creation
At this point, I discovered that those generated dataframes, even if they have very similar dimensions, have a much bigger size in Mb than the ones I used in previous simulations. This has a big impact on speed, and probably would create problems with the Shiny server memory limits. For this reason, I created a dictionary of the unique tokens associating each of them with a number, and then I created a numeric version of the trigrams and digrams dataframes. For some reason (lower number of characters, or maybe the numeric objects are memory saving compared to character object) those new dataframes are much lighter ($1/12$ in Mb).
```{r eval=F}
unigrams<-read.csv("unigrams.csv")
unigrams<-unigrams[!duplicated(unigrams$words),]
m<-list()
dir.create("df")
for(i in c("tags",LETTERS)) {
    a<-readRDS(paste0("models/merged/",i,".RDS"))
    a<-a %>% group_by(V1,V2) %>% filter(Frequency==max(Frequency)) %>% select(-4)
    a<-left_join(a, unigrams[,1:2], by=c("V3"="words"))
    a<-a %>% group_by(V1,V2) %>% filter(X==max(X)) %>% select(-4)
    m[[i]]<-a
}
saveRDS(m, "df/list.RDS")

digrams<-readRDS("digrams.RDS")
digrams<-digrams[order(digrams$Frequency, decreasing=T),]
digrams<-digrams %>% group_by(V1) %>% filter(Frequency==max(Frequency)) %>% select(-3)
digrams<-left_join(digrams, unigrams[,1:2], by=c("V2"="words"))
digrams<-digrams %>% group_by(V1) %>% filter(X==max(X)) %>% select(-3)
saveRDS(digrams, "df/digrams.RDS")
m<-readRDS("df/list.RDS")
digrams<-readRDS("df/digrams.RDS")

words<-unlist(m) #dictionary creation
dict<-data.frame(tags=unique(words), code=1:length(unique(words)))
dicvec<-dict$code
names(dicvec)<-dict$tags
saveRDS(dicvec, "df/dicvec.RDS")


to_dict<- function(df) {
  df<-ungroup(df)
  df<-left_join(df, dict, by=c("V1"="tags"))
  df<-left_join(df, dict, by=c("V2"="tags"))
  df<-left_join(df, dict, by=c("V3"="tags"))
  df<-select(df, c(4,5,6))
  names(df)<-c("V1", "V2", "V3")
  df
}

m<-lapply(m, to_dict)
digrams<-digrams %>% ungroup %>% left_join(dict, by=c("V1"="tags")) %>% left_join(dict, by=c("V2"="tags")) %>% select(c(3,4))
digrams<-digrams[complete.cases(digrams),]
saveRDS(m, "df/coded.RDS")
saveRDS(digrams, "df/digrams_coded.RDS")


```
### Prediction algorithm 
Finally, I want to show you the prediction function, which consists of few lines and requires 3 files loaded (a dictionary, the 2-grams dataframe in its numerical version and a list with the 27 3-grams dataframes in their numerical version, for a total size of 181 Mb (Shiny free server RAM limit is 1Gb):
```{r cache=T, echo=T}
dicvec<-readRDS("df/dicvec.RDS")
digrams<-readRDS("df/digrams_coded.RDS")
m<-readRDS("df/coded.RDS")
print(object.size(list(m, digrams, dicvec)), units="Mb")

word_predict<-function(a,b) {
  v2<-dicvec[toupper(b)]
  v1<-toupper(a)
if(!(substr(v1,1,1) %in% names(m))) {w<-as.integer(filter(digrams, code.x==v2)$code.y)}
else {
  l<-m[[substr(v1,1,1)]]
  v1=dicvec[v1]
  w<-as.integer(filter(l, V1==v1 & V2==v2)$V3)
}
if(length(w)==0) {
  w<-as.integer(filter(digrams, code.x==v2)$code.y)
}
if(length(w)==0) {w<-743}
names(dicvec[w])
}
```


## Model validation
The final step is to test the prediction algorithm using the validation dataset: even if we are in a field where there is no "correct answer", testing the probability of guessing the next word typed on a large collection of blogs, newspaper and tweets will be useful to compare this first model with other models I will build during the project, or with someone else's model.  
For this task, I processed the validation dataset in trigrams, following the same steps I used for the training dataset.  
Note that all lines in the validation dataset were already previously subjected to profanity filtering, tagging and tokenisation, but those processing steps will have to be included in the final prediction model, so that the user will be able to input number, weblink etc and get the expected output.

```{r eval=F}
for(i in 46:50) {
tokenized<-readRDS(paste0("Tokenized/chunk",i,".RDS"))
trigrams<-unlist(sapply(tokenized, trigrams_gen), recursive = F)
trigrams<-as.data.frame(do.call(rbind, trigrams))
saveRDS(trigrams, paste0("validation/chunk",i,".RDS"))
}

temp<-list.files("validation", full.names = T)
five<-lapply(temp, readRDS)
five<-do.call(rbind,five)
saveRDS(five, "validation.RDS")
```
```{r echo=T, cache=T}
validation<-readRDS("validation.RDS")
nrow(validation)
```
As the number of rows is too big to be tested in a reasonable amount of time, I only used a subsample of 2 millions of trigrams (21.6% of the total), and testing was completed in about 19 hours. The first 2 words in each trigram were passed to the prediction function, and the output was compared with the third word of each trigram.  
So, I calculated accuracy (percentage of words correctly guessed) and mean prediction time. In the table you can see a few lines of the testing output (column "V3" contains expected word, column "prediction" contains the output from the algorithm).
```{r eval=F}
validation<-readRDS("validation.RDS")
i<-sample(1:nrow(validation), 2000000)
validation<-validation[i,]
a<-Sys.time()
for(i in 1:nrow(validation)) {
  validation$prediction[i]<-word_predict(validation[i,]$V1, validation[i,]$V2)
}
b<-Sys.time()
validation$Correct_prediction<-validation$V3==validation$prediction
saveRDS(validation,"validation_results.RDS")
saveRDS(a, "start_time.RDS")
saveRDS(b, "stop_time.RDS")
```
```{r cache=T, echo=4:5}
validation<-readRDS("validation_results.RDS")
a<-readRDS("start_time.RDS")
b<-readRDS("stop_time.RDS")
round(sum(validation$Correct_prediction)/nrow(validation)*100,2) #Accuracy %
round(difftime(b,a, units="sec")/nrow(validation),4) # Mean time for prediction
```

<br/>
<div align="center">
```{r results="asis", cache=T}
print(xtable(validation[1:100,]), type="html")
```
</div>
<br/>

## Possible improvements
I am quite satisfied of my first attempt. With a runtime of 0.033 seconds, I think I'll be able to build a Shiny app that doesn't require a button to start computation, but does predictions in continuous, just like a smartphone keyboard does. I think this will still be possible after adding input tokenisation, profanity filtering and hopefully some extra computation to improve accuracy.  
In fact, the accuracy is not very high (16.44%). I don't know what a reasonable benchmark would be, but observing my mobile keyboard in my native language, it correctly guesses the word I want to type once every 3 or maybe 4 times. So, I want to try to enhance accuracy to 25% at least.  
Some of my ideas:  

-    First of all, as the introduction of a character/number dictionary allowed me to save memory and time, I will definitely try using a prediction with the 3 previous words.
-    Also, I could consider using some NLP methods to remove suffixes from words, thus reducing the number of rows in each dataframe.  
-   As previously mentioned, I could create some small dictionaries with the most common words in foreign languages and force the algorithm to look into those dictionaries when the word is not (or is rare, e.g. in the last 5% of frequency) in the English dictionary.  

I also thought about another approach, which is maybe more similar to the one suggested in the Capstone project (which proposes to manage OOV by smoothing probabilities, something quite different from my solution to OOV).  
This approach would rely on a dictionary and on 1,2,3 or more dataframes (depending on the number of words desired for prediction). Those dataframes should contain:  

-    In the first column one of the input words  
-    In the second column the top (e.g. 5) most frequent words at a given distance from that word (immediately following, or separeted by one word, or separated by 2 words...)  
-    In the third column the probability of that combination of words (the % of time that, in the Corpora data set, the first given word is associated with the second at that given distance).  

Then, the algorithm would sum up (maybe with some weights to give more importance to the nearest words) the probabilities for each possible predicted word and choose the most probable.  
To manage OOV, for each possible word in the 2nd column a line should be included in each dataframe in combination with a "OOV" tag in 1st column and a small (I should think about how small) probability in the 3rd.  
I would like to discover if this solution is worse or better than mine in terms of memory required, speed and accuracy, but this would mean to restart my job just after tokenization, so I'm not sure if I will do this (I will decide after reading my peers reports and looking at some Natural Language Prediction resources).
